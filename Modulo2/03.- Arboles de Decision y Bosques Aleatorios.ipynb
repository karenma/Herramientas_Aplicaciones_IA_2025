{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52abda8b",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#000047; padding: 30px; border-radius: 10px; color: white; text-align: center;\">\n",
    "    <img src='Figures/alinco.png' style=\"height: 100px; margin-bottom: 10px;\"/>\n",
    "    <h1> Arboles de Decision y Bosques Aleatorios</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a588b",
   "metadata": {},
   "source": [
    "## Arboles de Desición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fa6b1",
   "metadata": {},
   "source": [
    "Los árboles de decisión, también conocidos como modelos de árbol de clasificación y regresión (CART), son métodos basados en árboles para el aprendizaje automático supervisado. Los árboles de clasificación y de regresión simples son fáciles de usar e interpretar, pero no son competitivos con los mejores métodos de aprendizaje automático. Sin embargo, forman la base para el conjunto de modelos de ensamblaje como “bagged trees”, “random forest” y “boosted trees”, que aunque son menos interpretables, son muy precisos.\n",
    "\n",
    "Los modelos CART se puede definir en dos tipos de problemas\n",
    "\n",
    "**Árboles de clasificación:** la variable resultado es categórica y el métodos se utiliza para identificar la “clase” dentro de la cual es más probable que caiga nuestra variable resultado. Un ejemplo de un problema de tipo clasificación sería determinar quién se suscribirá o no a una plataforma digital; o quién se graduará o no de la escuela secundaria; o si una persona tiene cáncer o no.\n",
    "\n",
    "**Árboles de regressión:** la variable resultado es continua y el métodos se utiliza para predecir su valor. Un ejemplo de un problema de tipo regresión sería predecir los precios de venta de una casa residencial o el nivel de colesterol de una persona.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad515c2",
   "metadata": {},
   "source": [
    "Un árbol de decisión es una secuencia de operadores relacionales organizados como árbol donde:\n",
    "\n",
    "- Los atributos de un dato son evaluados desde la raiz hasta las hojas\n",
    "- Los nodos hoja (terminales) están asociados a una clase\n",
    "- Los nodos no-hoja están asociados a un operador lógico que divide los datos en dos o más conjuntos\n",
    "- El operador lógico o *split* se aplica sobre un atributo (feature) de los datos\n",
    "\n",
    "El siguiente diagrama ejemplifica el funcionamiento del árbol de decisión sobre un dataset con dos etiquetas y dos atributos (X y Z). \n",
    "\n",
    "<img src=\"Figures/tree.png\" width=\"600\">\n",
    "\n",
    "- La figura izquierda muestra un árbol de decisión binario con 5 nodos: 3 nodos hoja y 2 nodos de decisión.\n",
    "- La figura derecha muestra la partición que produce el árbol de decisión en el espacio de los datos. \n",
    "- Las separaciones o *splits* son siempre perpendiculares a los ejes de los datos (atributos).\n",
    "\n",
    "\n",
    "Entrenar el árbol de decisión es el proceso de escoger los atributos, operadores y umbrales de separación en los nodos de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52c53b",
   "metadata": {},
   "source": [
    "La función de costo más común para los árboles de regresión es la suma de los residuos al cuadrado,\n",
    "\n",
    "$$RSS = \\sum_{k=1}^{K}\\sum_{i \\ in A_k} (y_i - \\hat{y}_{A_k})^2$$\n",
    "\n",
    "Para árboles de clasificación, es el índice de Gini,\n",
    "\n",
    "$$ G=\\sum_{c=1}^C \\hat{p}_{kc} (1-\\hat{p}_{kc})$$\n",
    "\n",
    "y la entropía (información estadística)\n",
    "\n",
    "$$ E= - \\sum_{c=1}^C \\hat{p}_{kc} log(\\hat{p}_{kc})$$\n",
    "\n",
    "dónde $\\hat{p}_{kc}$ es la proporción de observaciones de entrenamiento en el nodo $k$ que son de clase $c$. Un nodo completamente puro en un árbol binario tendría $\\hat{p} \\in \\{0,1\\}$ y $G=E=0$.  Un nodo completamente impuro en un árbol binario tendría $\\hat{p}=0.5$ y $G=0.5^2*2 = 0.25$, y $D=- (0.5 \\cdot log(0.5))\\cdot 2 = 0.69$\n",
    "\n",
    "La ganacia de información para un nodo que separa un conjunto de datos $D$ en dos $D_{izq}$ y $D_{der}$ es\n",
    "\n",
    "$$\n",
    "G(D; D_{izq}, D_{der}) = H(D) - \\frac{|D_{izq}|}{|D|} H(D_{izq}) - \\frac{|D_{der}|}{|D|} H(D_{der})\n",
    "$$\n",
    "\n",
    "donde $|A|$ es la cardinalidad del subconjunto $A$ y \n",
    "\n",
    "$$\n",
    "H(A) = - \\sum_{y \\in \\mathcal{Y}} p(y|A) \\log p(y|A)\n",
    "$$\n",
    "\n",
    "es la entropía del subconjunto $A$. En la expresión anterior $p(y|A)$ es la frecuencia relativa de los ejemplos de clase $y$ dentro de $A$.\n",
    "\n",
    "**La entropía mide la “pureza” del subconjunto en términos de sus clases. El subconjunto más puro es aquel donde todos los elementos son de la misma clase. El nodo más impuro es aquel en donde hay igual cantidad de elementos de cada clase (uniforme).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7764d",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d92e4",
   "metadata": {},
   "source": [
    "Sea el siguiente arreglo las etiquetas de un subconjunto de 12 ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1,0,0,1,1,1,0,1,0,1,1,1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d1f4a",
   "metadata": {},
   "source": [
    "Asumiendo que el problema sólo tiene dos clases las frecuencias relativas son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, counts= np.unique(labels, return_counts=True)\n",
    "counts/len(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4df35",
   "metadata": {},
   "source": [
    "Y la entropía del conjunto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(subset_labels):\n",
    "    unique, counts= np.unique(subset_labels, return_counts=True)\n",
    "    frecuencias = counts/len(subset_labels)\n",
    "    entropia = -np.sum(frecuencias*np.log2(frecuencias +1e-16))\n",
    "    return entropia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd72f2",
   "metadata": {},
   "source": [
    "La entropía es máxima si hay igual cantidad de ejemplos de ambas clases (mínima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([1,1,1,0,0,0])\n",
    "entropy(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb824ea",
   "metadata": {},
   "source": [
    "y mínima si todos los ejemplos son de una clase  (máxima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf590fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([1,1,1,1,1,1])\n",
    "entropy(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83102f",
   "metadata": {},
   "source": [
    "**Extensión a más de dos clases**\n",
    "\n",
    "Si un nodo separa el conjunto en $k$ subconjuntos la regla es\n",
    "\n",
    "\n",
    "- En cada nodo se escoge el atributo que maximiza la ganancia de información.\n",
    "\n",
    "Consideremos el siguiente problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d965f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'tiempo' : ['soleado', 'soleado', 'soleado', 'lluvioso', 'lluvioso'],\n",
    "        'humedad' : ['baja','baja', 'alta','alta','alta'],\n",
    "        'temperatura': ['templado','caluroso', 'caluroso', 'templado', 'frio']}\n",
    "\n",
    "nodo = pd.DataFrame(data)\n",
    "nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d293b",
   "metadata": {},
   "source": [
    "donde queremos obtener un árbol de decisión que prediga el tiempo en función de la humedad y de la temperatura.\n",
    "\n",
    "Para decidir cual variable debe ir en el primer nodo comparamos sus ganancias de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(subset, feature):\n",
    "    subset_labels = subset['tiempo'].values\n",
    "    entropy_root = entropy(subset_labels)\n",
    "    entropy_nodes = []\n",
    "    for unique_label in subset[feature].unique():\n",
    "        split = subset.loc[subset[feature] == unique_label]\n",
    "        split_labels = split['tiempo'].values\n",
    "        entropy_nodes.append(entropy(split_labels)*len(split_labels)/len(subset_labels))\n",
    "    return entropy_root - sum(entropy_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bf8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['humedad','temperatura']:\n",
    "    print(f'La ganancia de información para {feature} es:{info_gain(nodo, feature)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04efe6",
   "metadata": {},
   "source": [
    "Temperatura tiene mayor ganancia que humedad, por lo tanto el primer nodo separador utiliza temperatura.\n",
    "\n",
    "Si separamos por temperatura tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo.loc[nodo['temperatura']=='frio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cd392",
   "metadata": {},
   "source": [
    "En el caso `frio` se produce un nodo con un sólo ejemplo. El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db026c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo.loc[nodo['temperatura']=='caluroso']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdaadc",
   "metadata": {},
   "source": [
    "En el caso `caluroso` se produce un nodo con \"puro\". El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba37311",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo.loc[nodo['temperatura']=='templado']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825a709",
   "metadata": {},
   "source": [
    "En el caso `templado` el nodo no es puro, debemos nuevamente escoger un atributo para separar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6509db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo_next = nodo.loc[nodo['temperatura']=='templado']\n",
    "for feature in ['temperatura', 'humedad']:\n",
    "    print(f'La ganancia de información para {feature} es:{info_gain(nodo_next, feature)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07b60b",
   "metadata": {},
   "source": [
    "Por lo tanto se escoge humedad, lo cual produce dos nodos puros (con un sólo ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de63396",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo_next.loc[nodo_next['humedad']=='baja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo_next.loc[nodo_next['humedad']=='alta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85af64c",
   "metadata": {},
   "source": [
    "El algoritmo sigue separando el dataset de forma recursiva hasta que todos los nodos sean puros o hasta que se supere una profundidad máxima previamente designada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547530a8",
   "metadata": {},
   "source": [
    "## Creación de la clase de Arboles de Desicion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # criterio de paro\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # mejor split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # Creación de los nodos hijos\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # Ganancio de información\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # pentropia del padre\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # creando un nodo hijo\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculando el peso medio de la entropia de los nodos hijos\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # Ganancia de información\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9ec27",
   "metadata": {},
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_breast_cancer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205337af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54764ac3-4972-440b-9e0a-8f96781cf4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bd159-f405-4bff-89c4-38d06a28e534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d4aaf8",
   "metadata": {},
   "source": [
    "## Ejemplo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_name = iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431d458-fd8a-4a13-a65e-1d51d6077a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.DataFrame(data=X, columns = iris.feature_names)\n",
    "df_iris['especie'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec933c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['especie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede30ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af7132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5e069b",
   "metadata": {},
   "source": [
    "## Implementación en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a76c3",
   "metadata": {},
   "source": [
    "El módulo [`tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) de scikit-learn tiene implementaciones de árboles de decisión para problemas de clasificación y regresión. Nos enfocaremos en la primera.\n",
    "\n",
    "Los principales argumentos de [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) son:\n",
    "\n",
    "- `criterion`: El criterio que se utiliza para escoger los *splits*, las opciones son `'gini'` y `'entropy'`\n",
    "- `max_depth`: Límite para la profundidad máxima del árbol\n",
    "- `min_samples_split`: El número mínimo de ejemplos en un nodo para realizar un *split*\n",
    "- `min_samples_leaf`: El número mínimo de ejemplos que pueden estar en un nodo hoja\n",
    "- `min_impurity_decrease`: La disminución de pureza mínima en un nodo para realizar un *split*\n",
    "- `class_weight`: Permite asignar ponderación a las clases, es de utilidad si se tienen clases medianamente desbalanceadas\n",
    "- `max_features`: El número máximo de atributos a considerar en cada *split*\n",
    "\n",
    "\n",
    "Si se utilizan los argumentos (hiperparámetros) por defecto el árbol crecera hasta que sus nodos sean todos puros. Esto en general produce árboles de gran profundidad (muy capaces de sobreajustarse). \n",
    "\n",
    "Se puede limitar el tamaño de un árbol aumentando `min_samples_leaf` y/o `min_samples_split`, o disminuyendo `max_depth`.\n",
    "\n",
    "\n",
    "\n",
    "Los principales métodos son:\n",
    "\n",
    "- `predict(X)`: Retorna la clase predicha\n",
    "- `predict_proba(X)`: Retorna las probabilidades de pertenecer a cada una de las clases\n",
    "- `score(X,y)`: Retorna el *accuracy* de clasificación\n",
    "- `get_params()`: Retorna los nombres de los parámetros\n",
    "\n",
    "Además tiene algunos métodos no compartidos con otros estimadores como\n",
    "\n",
    "- `get_depth()`: Retorna la profunidad del árbol aprendido\n",
    "- `get_n_leaves()`: Retorna la cantidad de nodos hoja del árbol aprendida\n",
    "- `apply(X)`: Retorna el índice de la hoja que predice cada ejemplo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b10bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "522effce",
   "metadata": {},
   "source": [
    "Podemos utilizar la función [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) para obtener una visualización del árbol de decisión. En cada nodo se muestra:\n",
    "\n",
    "- El atributo y umbral seleccionados.\n",
    "- El valor del criterio (índice de gini).\n",
    "- La cantidad de ejemplos que entraron al nodo.\n",
    "- La cantidad de ejemplos que entraron al nodo separados por clase (en este caso tres).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creación del modelo\n",
    "model_iris = DecisionTreeClassifier(criterion='entropy')\n",
    "model_iris.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c5177-2b06-4a66-a688-7de912551646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405109a0-2f9c-40bd-9a0f-f19904889400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c020c4e8-b522-4665-bf32-9132b0db95f0",
   "metadata": {},
   "source": [
    "## Bosques Aleatorios (Random Forest)\n",
    "\n",
    "**Random Forest** es un modelo de conjunto (ensemble) que combina múltiples árboles de decisión para mejorar la precisión y reducir el sobreajuste.\n",
    "\n",
    "<b>Algoritmo</b>\n",
    "<ul>\n",
    "<li>Se construyen muchos árboles de decisión independientes.</li>\n",
    "<li>Cada árbol se entrena con una muestra aleatoria de los datos (bootstrap) y una selección aleatoria de características en cada división (feature bagging).</li>\n",
    "<li>Para clasificación, cada árbol vota por una clase y la clase final es la más votada (mayoría).</li>\n",
    "<li>Para regresión, se promedian las predicciones de todos los árboles.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Proceso de construcción:</b>\n",
    "<ol>\n",
    "<li>Se generan <i>N</i> subconjuntos de datos mediante muestreo con reemplazo (bootstrap).</li>\n",
    "<li>Para cada subconjunto, se construye un árbol de decisión usando solo un subconjunto aleatorio de características en cada nodo.</li>\n",
    "<li>Las predicciones de todos los árboles se combinan (votación o promedio).</li>\n",
    "</ol>\n",
    "\n",
    "<b>Ejemplo de aplicación:</b> Random Forest es ampliamente usado en tareas de clasificación (diagnóstico médico, detección de fraude, etc.) y regresión (predicción de precios, estimación de demanda, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4770a-2ff3-4a10-9e14-34c6f0235694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from HyAIA import HyAIA as hya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58390b3c-61b0-42ab-9153-c449f65ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f4986-4a9f-4653-bf23-84241488678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = hya(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181e6b3-c620-4fcc-9896-78356816fa69",
   "metadata": {},
   "source": [
    "#### Preprocesamiento\n",
    "\n",
    "Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c07db-345c-4cd8-8a7c-5f40666fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59ca80-59ea-4bc5-84e5-792a68b972a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando outliers (forma básica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63daf2-1154-444b-8919-bee71fbf8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos imbalanceados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a657f-8a81-47e8-918f-33a0ad283861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f1133-080d-49ca-989d-101db2a629fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195c959-d5bc-475a-84f5-4e5374431860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de evaluación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49738c0e-11c7-40b6-80c9-35dd08c35538",
   "metadata": {},
   "source": [
    "#### Modelo de arbol de desición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb5b68-3c2f-47eb-81ec-98151a50676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a794f1e-bf4c-46de-a67d-6a097cd9959a",
   "metadata": {},
   "source": [
    "#### Modelo de Arboles Aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274868a4-1527-42f3-afb5-94671704fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44f188-e08e-4520-8b48-fb6200b7b323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f714692-553b-4a63-aa11-913b0b6ff0d8",
   "metadata": {},
   "source": [
    "<b>Resumen:</b>\n",
    "<ul>\n",
    "<li>Reduce el riesgo de sobreajuste que tienen los árboles individuales.</li>\n",
    "<li>Generaliza mejor en datos nuevos.</li>\n",
    "<li>Puede manejar grandes cantidades de variables y datos.</li>\n",
    "<li>Permite estimar la importancia de cada variable.</li>\n",
    "</ul>\n",
    "\n",
    "<b>algunas desventajas:</b>\n",
    "<ul>\n",
    "<li>Menos interpretable que un solo árbol de decisión.</li>\n",
    "<li>Puede ser más lento en entrenamiento y predicción si el número de árboles es grande.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bced90-75d6-429b-b5ae-0efec30c0480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
